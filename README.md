# ClassificaÃ§Ã£o de Lixo ReciclÃ¡vel com Deep Learning

## DescriÃ§Ã£o do Projeto

Este projeto implementa e compara dois modelos de aprendizado profundo para classificaÃ§Ã£o automÃ¡tica de lixo reciclÃ¡vel utilizando o dataset **TrashNet**. O objetivo Ã© desenvolver soluÃ§Ãµes robustas para auxiliar na separaÃ§Ã£o automatizada de resÃ­duos reciclÃ¡veis, contribuindo para prÃ¡ticas ambientalmente sustentÃ¡veis.

## Objetivo

Desenvolver e comparar modelos de classificaÃ§Ã£o de imagens para distinguir entre 6 categorias de resÃ­duos:
- **Cardboard** (PapelÃ£o)
- **Glass** (Vidro) 
- **Metal** (Metal)
- **Paper** (Papel)
- **Plastic** (PlÃ¡stico)
- **Trash** (Lixo comum)

## Dataset

**TrashNet Dataset:**
- **Total de imagens:** 2.527
- **Classes:** 6 categorias de resÃ­duos
- **ResoluÃ§Ã£o:** Redimensionada para 160Ã—160 pixels
- **DivisÃ£o:**
  - Treino: 2.022 imagens (80%)
  - ValidaÃ§Ã£o: 253 imagens (10%)  
  - Teste: 252 imagens (10%)

### DistribuiÃ§Ã£o por Classe:
| Classe | Quantidade | Percentual |
|--------|------------|------------|
| Paper | 594 | 23.5% |
| Glass | 501 | 19.8% |
| Plastic | 482 | 19.1% |
| Cardboard | 403 | 16.0% |
| Metal | 410 | 16.2% |
| Trash | 137 | 5.4% |

## Modelos Implementados

### 1. CNN Baseline (Arquitetura do Zero)

**Arquitetura:**
- 3 blocos convolucionais (32, 64, 128 filtros)
- BatchNormalization + ReLU + MaxPooling2D
- Dropout (0.3) apÃ³s cada bloco convolucional
- GlobalAveragePooling2D
- Dense(256) + Dropout(0.5) + Dense(6)

**ConfiguraÃ§Ãµes de Treino:**
- **Otimizador:** AdamW (learning_rate=1e-4, weight_decay=1e-4)
- **Loss:** Sparse Categorical Crossentropy
- **Ã‰pocas:** 20 (early stopping)
- **Callbacks:** ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
- **ParÃ¢metros:** 128.486 (500KB)

### 2. MobileNetV2 Transfer Learning

**Arquitetura:**
- **Base:** MobileNetV2 prÃ©-treinada (ImageNet)
- **EstratÃ©gia:** Transfer Learning em 2 fases
  - Fase 1: Base congelada (8 Ã©pocas)
  - Fase 2: Fine-tuning das Ãºltimas 40 camadas (7 Ã©pocas)
- **CabeÃ§alho:** GlobalAveragePooling2D + Dropout(0.2) + Dense(6)

**ConfiguraÃ§Ãµes de Treino:**
- **Otimizador:** AdamW (1e-4, weight_decay=1e-4)
- **Loss:** Sparse Categorical Crossentropy
- **Callbacks:** EarlyStopping

## ðŸ† Resultados dos Experimentos

### ComparaÃ§Ã£o Geral dos Modelos

| Modelo | AcurÃ¡cia Final | ParÃ¢metros | Tempo/Ã‰poca |
|--------|----------------|------------|-------------|
| **MobileNetV2 TL** | **78.71%** | ~2.3M | ~30s |
| **CNN Baseline** | 57.83% | 128K | ~70s |

** DiferenÃ§a de Performance:** 20.88 pontos percentuais a favor do Transfer Learning

### Resultados Detalhados por Classe

#### MobileNetV2 Transfer Learning (Melhor Modelo)
| Classe | Precision | Recall | F1-Score | Support |
|--------|-----------|--------|----------|---------|
| Cardboard | 81.82% | 79.41% | 80.60% | 34 |
| Glass | 78.72% | 72.55% | 75.51% | 51 |
| Metal | 88.24% | 73.17% | 80.00% | 41 |
| Paper | 83.87% | 85.25% | 84.55% | 61 |
| Plastic | 70.59% | 80.00% | 75.00% | 45 |
| Trash | 54.55% | 70.59% | 61.54% | 17 |

#### CNN Baseline
| Classe | Precision | Recall | F1-Score | Support |
|--------|-----------|--------|----------|---------|
| Cardboard | 61.36% | 79.41% | 69.23% | 34 |
| Glass | 61.76% | 44.68% | 51.85% | 47 |
| Metal | 75.76% | 59.52% | 66.67% | 42 |
| Paper | 49.02% | 83.33% | 61.73% | 60 |
| Plastic | 66.67% | 48.00% | 55.81% | 50 |
| Trash | 0.00% | 0.00% | 0.00% | 16 |

### AnÃ¡lise de Performance

#### âœ… Pontos Fortes do MobileNetV2:
- **Excelente performance geral** (78.71% acurÃ¡cia)
- **ConvergÃªncia rÃ¡pida** devido ao Transfer Learning
- **Boa generalizaÃ§Ã£o** em todas as classes
- **EficiÃªncia computacional** durante inferÃªncia

#### âš ï¸ LimitaÃ§Ãµes Identificadas:
- **Classe "Trash" mais desafiadora** (menor recall)
- **Desbalanceamento do dataset** afeta classes minoritÃ¡rias
- **Paper vs Cardboard** apresentam alguma confusÃ£o

#### ðŸ“Š CNN Baseline:
- **Performance limitada** mas aceitÃ¡vel para arquitetura simples
- **Dificuldade com classe "Trash"** (0% precision/recall)
- **Overfitting mais pronunciado** devido ao tamanho limitado do dataset

## TÃ©cnicas de OtimizaÃ§Ã£o Aplicadas

### Data Augmentation:
- RandomFlip (horizontal)
- RandomRotation (Â±3Â°)
- RandomZoom (Â±5%)

### RegularizaÃ§Ã£o:
- Dropout nas camadas convolucionais (0.3)
- Dropout na camada densa (0.5)
- Weight Decay (AdamW)

### EstratÃ©gias de Treino:
- Early Stopping (patience=6)
- ReduceLROnPlateau
- ModelCheckpoint (best validation accuracy)

## ðŸ“ Estrutura de Arquivos

```
ðŸ“¦ results/
 â”£ ðŸ¤– models/
 â”ƒ â”— cnn_baseline_best.keras          # Melhor modelo CNN baseline
 â”£ ðŸ“Š plots/
 â”ƒ â”£ accuracy/                        # Curvas de acurÃ¡cia por modelo
 â”ƒ â”£ loss/                           # Curvas de loss por modelo  
 â”ƒ â”— confusion_matrices/             # Matrizes de confusÃ£o
 â”£ ðŸ“ˆ history/
 â”ƒ â”£ cnn_baseline_history.csv        # HistÃ³rico treino CNN
 â”ƒ â”£ mobilenetv2_tl_freeze_history.csv    # HistÃ³rico fase frozen
 â”ƒ â”— mobilenetv2_tl_finetune_history.csv  # HistÃ³rico fine-tuning
 â”— ðŸ“‹ reports/
   â”£ models_comparison.csv            # ComparaÃ§Ã£o entre modelos
   â”£ class_report_cnn_baseline.csv    # MÃ©tricas detalhadas CNN
   â”— class_report_mobilenetv2_tl.csv  # MÃ©tricas detalhadas MobileNetV2
```

### OrganizaÃ§Ã£o dos Resultados:
- **`results/models/`** - Modelos treinados salvos (.keras)
- **`results/plots/`** - Todas as visualizaÃ§Ãµes (grÃ¡ficos e matrizes)
- **`results/history/`** - HistÃ³ricos de treino em CSV
- **`results/reports/`** - RelatÃ³rios de mÃ©tricas e comparaÃ§Ãµes

## ï¿½ Como Executar

### 1. **Clonar repositÃ³rio:**
```bash
git clone https://github.com/AnaJuvencio/Aprendizado_Profundo_Reconhecimento_Visual.git
cd Aprendizado_Profundo_Reconhecimento_Visual
```

### 2. **Baixar o Dataset TrashNet:**
```bash
# OpÃ§Ã£o 1: Clonar repositÃ³rio do TrashNet
git clone https://github.com/garythung/trashnet.git trashnet-master

# OpÃ§Ã£o 2: Download direto (ZIP)
# Acesse: https://github.com/garythung/trashnet
# Extraia para: trashnet-master/
```

**âš ï¸ Estrutura esperada:**
```
ðŸ“¦ Projeto/
 â”— ðŸ“‚ trashnet-master/
   â”— ðŸ“‚ data/
     â”— ðŸ“‚ dataset-resized/
       â”£ ðŸ“ cardboard/
       â”£ ðŸ“ glass/
       â”£ ðŸ“ metal/
       â”£ ðŸ“ paper/
       â”£ ðŸ“ plastic/
       â”— ðŸ“ trash/
```

### 3. **Instalar dependÃªncias:**
```bash
pip install tensorflow scikit-learn pandas matplotlib
```

### 4. **Executar notebook:**
```bash
jupyter notebook Projeto_Aprendizado_Profundo.ipynb
```

### 5. **Explorar resultados:**
```bash
# Visualizar estrutura de resultados
ls results/

# Carregar modelo salvo
python -c "from tensorflow import keras; model = keras.models.load_model('results/models/cnn_baseline_best.keras')"
```

## ðŸ“¥ Sobre o Dataset

**Por que o dataset nÃ£o estÃ¡ no repositÃ³rio?**
- **Tamanho:** ~500MB+ (excede limites do GitHub)
- **Boa prÃ¡tica:** Datasets grandes devem ser baixados separadamente  
- **Performance:** MantÃ©m o repositÃ³rio leve e clones rÃ¡pidos

**TrashNet Dataset:**
- **Fonte original:** https://github.com/garythung/trashnet
- **LicenÃ§a:** Consulte o repositÃ³rio original
- **Tamanho:** 2.527 imagens (160x160px redimensionadas)

## ï¿½ðŸ”§ ConfiguraÃ§Ã£o do Ambiente

### DependÃªncias:
```python
tensorflow>=2.16.1
scikit-learn
pandas
matplotlib
numpy
```

### Hardware Utilizado:
- **CPU:** Processamento em CPU (sem GPU)
- **RAM:** ConfiguraÃ§Ã£o padrÃ£o
- **Batch Size:** 16 (otimizado para CPU)

## ðŸ“ˆ ConclusÃµes e Insights

### Principais Achados:

1. **Transfer Learning Superior:** MobileNetV2 supera significativamente a CNN baseline (+20.88%)

2. **EficiÃªncia do PrÃ©-treinamento:** Utilizar features prÃ©-treinadas no ImageNet acelera convergÃªncia e melhora generalizaÃ§Ã£o

3. **Desafios do Desbalanceamento:** Classe "Trash" (5.4% do dataset) apresenta maior dificuldade

4. **RegularizaÃ§Ã£o Efetiva:** Dropout e weight decay preveniram overfitting severo

### Melhorias Futuras:

1. **Balanceamento de Classes:**
   - Implementar class weights
   - TÃ©cnicas de data augmentation especÃ­ficas
   - Coleta de mais dados para classes minoritÃ¡rias

2. **Arquiteturas AvanÃ§adas:**
   - EfficientNet
   - Vision Transformers
   - Ensemble methods

3. **OtimizaÃ§Ãµes:**
   - Mixed precision training
   - Learning rate scheduling avanÃ§ado
   - Cross-validation

4. **AplicaÃ§Ã£o PrÃ¡tica:**
   - Deploy em dispositivos mobile
   - API para classificaÃ§Ã£o em tempo real
   - Interface web para usuÃ¡rios finais


## MÃ©tricas de Sucesso AlcanÃ§adas

- âœ… **AcurÃ¡cia > 75%** com MobileNetV2 Transfer Learning
- âœ… **Modelo baseline funcional** com arquitetura simples
- âœ… **ConvergÃªncia estÃ¡vel** sem overfitting severo
- âœ… **DocumentaÃ§Ã£o completa** e reprodutÃ­vel
- âœ… **AnÃ¡lise comparativa detalhada** entre abordagens


## Como Obter o Dataset

**Download do TrashNet:**
   - Acesse: https://github.com/garythung/trashnet
   - Clone: `git clone https://github.com/garythung/trashnet.git`
   - Ou baixe o ZIP diretamente

---

**Desenvolvido por:** AnaJuvencio  
**Data:** Outubro 2025  
**Framework:** TensorFlow 2.16.1

